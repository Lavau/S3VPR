2024-05-01 08:21:40   Arguments: Namespace(aggregator_name='gemhead', backbone_name='vit_b_32', device='cuda', dim=768, epochs_num=80, foundation_model_path='pth/dinov2_vitb14_pretrain.pth', infer_batch_size=300, kernel_size=3, lr=0.0001, mlp_ratio=1, nc=4096, num_trainable_blocks=4, num_workers=4, optim='adam', patience=3, recall_values=[1, 5, 10, 100], resize=[224, 224], save_dir='logs/2024-05-01_08-21-40', seed=0, train_batch_size=160, val_set_names=['msls_val'])
2024-05-01 08:21:40   The outputs are being saved in logs/2024-05-01_08-21-40
2024-05-01 08:21:40   validation dataset names:['msls_val']
2024-05-01 08:21:40   Using 2 GPUs and 64 CPUs
2024-05-01 08:21:46   GeoLocalizationNet(
  (backbone): ViT(
    (model): VisionTransformer(
      (conv_proj): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))
      (encoder): Encoder(
        (dropout): Dropout(p=0.0, inplace=False)
        (layers): Sequential(
          (encoder_layer_0): EncoderBlock(
            (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (self_attention): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
            (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (mlp): MLPBlock(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=3072, out_features=768, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (encoder_layer_1): EncoderBlock(
            (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (self_attention): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
            (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (mlp): MLPBlock(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=3072, out_features=768, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (encoder_layer_2): EncoderBlock(
            (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (self_attention): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
            (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (mlp): MLPBlock(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=3072, out_features=768, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (encoder_layer_3): EncoderBlock(
            (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (self_attention): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
            (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (mlp): MLPBlock(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=3072, out_features=768, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (encoder_layer_4): EncoderBlock(
            (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (self_attention): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
            (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (mlp): MLPBlock(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=3072, out_features=768, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (encoder_layer_5): EncoderBlock(
            (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (self_attention): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
            (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (mlp): MLPBlock(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=3072, out_features=768, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (encoder_layer_6): EncoderBlock(
            (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (self_attention): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
            (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (mlp): MLPBlock(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=3072, out_features=768, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (encoder_layer_7): EncoderBlock(
            (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (self_attention): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
            (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (mlp): MLPBlock(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=3072, out_features=768, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (encoder_layer_8): EncoderBlock(
            (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (self_attention): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
            (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (mlp): MLPBlock(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=3072, out_features=768, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (encoder_layer_9): EncoderBlock(
            (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (self_attention): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
            (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (mlp): MLPBlock(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=3072, out_features=768, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (encoder_layer_10): EncoderBlock(
            (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (self_attention): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
            (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (mlp): MLPBlock(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=3072, out_features=768, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (encoder_layer_11): EncoderBlock(
            (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (self_attention): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
            (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (mlp): MLPBlock(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=3072, out_features=768, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (heads): Sequential(
        (head): Linear(in_features=768, out_features=1000, bias=True)
      )
    )
  )
  (aggregator): GeMHead(
    (pool): GeneralizedMeanPooling(3.0, output_size=1)
    (fc): Linear(in_features=768, out_features=4096, bias=True)
  )
)
2024-05-01 08:21:47   Val set: < MSLS, msls-val - #database: 18871; #queries: 747 >
2024-05-01 08:21:47   Start training epoch: 00
2024-05-01 08:35:03   Finished epoch 00 in 0:13:15, average epoch loss = 1.2774
2024-05-01 08:35:40   Ranking results: msls_val R@1: 0.3811, R@5: 0.5230, R@10: 0.5811, R@15: 0.6081, R@20: 0.6378, R@50: 0.7095, R@100: 0.7703
2024-05-01 08:35:42   Improved: previous msls-val best R@1 = 0.0000, current R@1 = 0.3811
2024-05-01 08:35:42   Start training epoch: 01
2024-05-01 08:49:02   Finished epoch 01 in 0:13:20, average epoch loss = 1.1159
2024-05-01 08:49:45   Ranking results: msls_val R@1: 0.4432, R@5: 0.5541, R@10: 0.6135, R@15: 0.6392, R@20: 0.6568, R@50: 0.7122, R@100: 0.7689
2024-05-01 08:49:52   Improved: previous msls-val best R@1 = 0.3811, current R@1 = 0.4432
2024-05-01 08:49:52   Start training epoch: 02
2024-05-01 09:03:02   Finished epoch 02 in 0:13:09, average epoch loss = 1.0438
2024-05-01 09:03:46   Ranking results: msls_val R@1: 0.4419, R@5: 0.5703, R@10: 0.6162, R@15: 0.6459, R@20: 0.6730, R@50: 0.7338, R@100: 0.7743
2024-05-01 09:03:50   Not improved: 1 / 3: msls-val best R@1 = 0.4432, current R@1 = 0.4419
2024-05-01 09:03:50   Start training epoch: 03
2024-05-01 09:16:58   Finished epoch 03 in 0:13:07, average epoch loss = 0.9884
2024-05-01 09:17:41   Ranking results: msls_val R@1: 0.4459, R@5: 0.5595, R@10: 0.6135, R@15: 0.6405, R@20: 0.6608, R@50: 0.7216, R@100: 0.7703
2024-05-01 09:17:48   Improved: previous msls-val best R@1 = 0.4432, current R@1 = 0.4459
2024-05-01 09:17:48   Start training epoch: 04
2024-05-01 09:30:39   Finished epoch 04 in 0:12:51, average epoch loss = 0.9660
2024-05-01 09:31:23   Ranking results: msls_val R@1: 0.4473, R@5: 0.5757, R@10: 0.6378, R@15: 0.6703, R@20: 0.7000, R@50: 0.7608, R@100: 0.8122
2024-05-01 09:31:30   Improved: previous msls-val best R@1 = 0.4459, current R@1 = 0.4473
2024-05-01 09:31:30   Start training epoch: 05
2024-05-01 09:44:36   Finished epoch 05 in 0:13:05, average epoch loss = 0.9410
2024-05-01 09:45:20   Ranking results: msls_val R@1: 0.4608, R@5: 0.5757, R@10: 0.6432, R@15: 0.6676, R@20: 0.6851, R@50: 0.7608, R@100: 0.8108
2024-05-01 09:45:27   Improved: previous msls-val best R@1 = 0.4473, current R@1 = 0.4608
2024-05-01 09:45:27   Start training epoch: 06
2024-05-01 09:58:34   Finished epoch 06 in 0:13:06, average epoch loss = 0.9175
2024-05-01 09:59:17   Ranking results: msls_val R@1: 0.4419, R@5: 0.5595, R@10: 0.6243, R@15: 0.6541, R@20: 0.6703, R@50: 0.7554, R@100: 0.8068
2024-05-01 09:59:21   Not improved: 1 / 3: msls-val best R@1 = 0.4608, current R@1 = 0.4419
2024-05-01 09:59:21   Start training epoch: 07
2024-05-01 10:12:32   Finished epoch 07 in 0:13:11, average epoch loss = 0.9092
2024-05-01 10:13:14   Ranking results: msls_val R@1: 0.4419, R@5: 0.5581, R@10: 0.6068, R@15: 0.6392, R@20: 0.6649, R@50: 0.7419, R@100: 0.8027
2024-05-01 10:13:18   Not improved: 2 / 3: msls-val best R@1 = 0.4608, current R@1 = 0.4419
2024-05-01 10:13:18   Start training epoch: 08
2024-05-01 10:26:13   Finished epoch 08 in 0:12:55, average epoch loss = 0.8980
2024-05-01 10:26:57   Ranking results: msls_val R@1: 0.4365, R@5: 0.5635, R@10: 0.6054, R@15: 0.6297, R@20: 0.6581, R@50: 0.7405, R@100: 0.7946
2024-05-01 10:27:01   Not improved: 3 / 3: msls-val best R@1 = 0.4608, current R@1 = 0.4365
2024-05-01 10:27:01   Performance did not improve for 3 epochs. Stop training.
2024-05-01 10:27:01   Best  msls-val best R@1 = 0.4608
2024-05-01 10:27:01   Trained for 09 epochs, in total in 2:05:20
2024-05-01 10:27:01   Val set: < MSLS, msls-val - #database: 18871; #queries: 747 >
2024-05-01 10:27:01   Val set: < Tokyo247Dataset, Tokyo - #database: 75984; #queries: 423 >
2024-05-01 10:27:01   Val set: < SPEDDataset, SPED - #database: 607; #queries: 607 >
2024-05-01 10:27:01   Val set: < NordlandDataset, nordland - #database: 27592; #queries: 2760 >
2024-05-01 10:27:01   Val set: < WholeDatasetFromStruct, pitts30k_val - #database: 10000; #queries: 7608 >
2024-05-01 10:27:01   Val set: < WholeDatasetFromStruct, pitts30k_test - #database: 10000; #queries: 6816 >
2024-05-01 10:27:02   Val set: < WholeDatasetFromStruct, pitts250k_test - #database: 83952; #queries: 8280 >
2024-05-01 10:27:02   Val set: < WholeDatasetFromStruct, pitts250k_val - #database: 78648; #queries: 7608 >
2024-05-01 10:27:02   Test *best* model on test set
2024-05-01 10:27:48   Ranking results: msls_val R@1: 0.4608, R@5: 0.5757, R@10: 0.6432, R@15: 0.6676, R@20: 0.6851, R@50: 0.7608, R@100: 0.8108
2024-05-01 10:34:19   Ranking results: tokyo247 R@1: 0.0662, R@5: 0.0969, R@10: 0.1040, R@15: 0.1182, R@20: 0.1206, R@50: 0.1560, R@100: 0.2435
2024-05-01 10:34:24   Ranking results: SPED R@1: 0.3641, R@5: 0.5058, R@10: 0.5881, R@15: 0.6359, R@20: 0.6656, R@50: 0.7562, R@100: 0.8451
2024-05-01 10:35:36   Ranking results: nordland R@1: 0.0232, R@5: 0.0442, R@10: 0.0616, R@15: 0.0717, R@20: 0.0797, R@50: 0.1174, R@100: 0.1620
2024-05-01 10:36:19   Ranking results: pitts30k_val R@1: 0.3942, R@5: 0.5047, R@10: 0.5795, R@15: 0.6448, R@20: 0.6944, R@50: 0.8340, R@100: 0.9105
2024-05-01 10:37:07   Ranking results: pitts30k_test R@1: 0.5072, R@5: 0.6601, R@10: 0.7289, R@15: 0.7628, R@20: 0.7901, R@50: 0.8703, R@100: 0.9170
2024-05-01 10:41:54   Ranking results: pitts250k_test R@1: 0.3721, R@5: 0.4332, R@10: 0.4571, R@15: 0.4726, R@20: 0.4847, R@50: 0.5289, R@100: 0.5720
2024-05-01 10:46:14   Ranking results: pitts250k_val R@1: 0.3078, R@5: 0.4058, R@10: 0.4299, R@15: 0.4453, R@20: 0.4570, R@50: 0.5296, R@100: 0.6174
2024-05-01 10:46:14   Test *last* model on test set
2024-05-01 10:47:01   Ranking results: msls_val R@1: 0.4365, R@5: 0.5635, R@10: 0.6054, R@15: 0.6297, R@20: 0.6581, R@50: 0.7405, R@100: 0.7946
2024-05-01 10:53:27   Ranking results: tokyo247 R@1: 0.0473, R@5: 0.0733, R@10: 0.0875, R@15: 0.0946, R@20: 0.0993, R@50: 0.1277, R@100: 0.1868
2024-05-01 10:53:32   Ranking results: SPED R@1: 0.3394, R@5: 0.4860, R@10: 0.5420, R@15: 0.5947, R@20: 0.6277, R@50: 0.7331, R@100: 0.8204
2024-05-01 10:54:45   Ranking results: nordland R@1: 0.0232, R@5: 0.0453, R@10: 0.0587, R@15: 0.0670, R@20: 0.0739, R@50: 0.1087, R@100: 0.1511
2024-05-01 10:55:35   Ranking results: pitts30k_val R@1: 0.3582, R@5: 0.4746, R@10: 0.5497, R@15: 0.6180, R@20: 0.6701, R@50: 0.8215, R@100: 0.9017
2024-05-01 10:56:22   Ranking results: pitts30k_test R@1: 0.5000, R@5: 0.6293, R@10: 0.6912, R@15: 0.7289, R@20: 0.7551, R@50: 0.8440, R@100: 0.8976
2024-05-01 11:01:03   Ranking results: pitts250k_test R@1: 0.3557, R@5: 0.4066, R@10: 0.4266, R@15: 0.4399, R@20: 0.4454, R@50: 0.4802, R@100: 0.5158
2024-05-01 11:05:19   Ranking results: pitts250k_val R@1: 0.3010, R@5: 0.3595, R@10: 0.3772, R@15: 0.3912, R@20: 0.4017, R@50: 0.4479, R@100: 0.5273
